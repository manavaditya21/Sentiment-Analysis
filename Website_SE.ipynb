{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3uBLNLlIxgq",
        "outputId": "21599668-31d0-4527-caa4-0a5fbf785637",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.10/dist-packages (0.2.8)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.12.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.1)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.9.4)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.31.0)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.11)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (5.1.2)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2024.2.2)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.0.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.14.0)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article text saved for blackassign0001\n",
            "Article text saved for blackassign0002\n",
            "Article text saved for blackassign0003\n",
            "Article text saved for blackassign0004\n",
            "Article text saved for blackassign0005\n",
            "Article text saved for blackassign0006\n",
            "Article text saved for blackassign0007\n",
            "Article text saved for blackassign0008\n",
            "Article text saved for blackassign0009\n",
            "Article text saved for blackassign0010\n",
            "Article text saved for blackassign0011\n",
            "Article text saved for blackassign0012\n",
            "Article text saved for blackassign0013\n",
            "Article text saved for blackassign0014\n",
            "Article text saved for blackassign0015\n",
            "Article text saved for blackassign0016\n",
            "Article text saved for blackassign0017\n",
            "Article text saved for blackassign0018\n",
            "Article text saved for blackassign0019\n",
            "Article text saved for blackassign0020\n",
            "Article text saved for blackassign0021\n",
            "Article text saved for blackassign0022\n",
            "Article text saved for blackassign0023\n",
            "Article text saved for blackassign0024\n",
            "Article text saved for blackassign0025\n",
            "Article text saved for blackassign0026\n",
            "Article text saved for blackassign0027\n",
            "Article text saved for blackassign0028\n",
            "Article text saved for blackassign0029\n",
            "Article text saved for blackassign0030\n",
            "Article text saved for blackassign0031\n",
            "Article text saved for blackassign0032\n",
            "Article text saved for blackassign0033\n",
            "Article text saved for blackassign0034\n",
            "Article text saved for blackassign0035\n",
            "Error extracting article from https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/: Article `download()` failed with 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/ on URL https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
            "Unable to extract article text for blackassign0036, created empty file.\n",
            "Article text saved for blackassign0037\n",
            "Article text saved for blackassign0038\n",
            "Article text saved for blackassign0039\n",
            "Article text saved for blackassign0040\n",
            "Article text saved for blackassign0041\n",
            "Article text saved for blackassign0042\n",
            "Article text saved for blackassign0043\n",
            "Article text saved for blackassign0044\n",
            "Article text saved for blackassign0045\n",
            "Article text saved for blackassign0046\n",
            "Article text saved for blackassign0047\n",
            "Article text saved for blackassign0048\n",
            "Error extracting article from https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/: Article `download()` failed with 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/ on URL https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
            "Unable to extract article text for blackassign0049, created empty file.\n",
            "Article text saved for blackassign0050\n",
            "Article text saved for blackassign0051\n",
            "Article text saved for blackassign0052\n",
            "Article text saved for blackassign0053\n",
            "Article text saved for blackassign0054\n",
            "Article text saved for blackassign0055\n",
            "Article text saved for blackassign0056\n",
            "Article text saved for blackassign0057\n",
            "Article text saved for blackassign0058\n",
            "Article text saved for blackassign0059\n",
            "Article text saved for blackassign0060\n",
            "Article text saved for blackassign0061\n",
            "Article text saved for blackassign0062\n",
            "Article text saved for blackassign0063\n",
            "Article text saved for blackassign0064\n",
            "Article text saved for blackassign0065\n",
            "Article text saved for blackassign0066\n",
            "Article text saved for blackassign0067\n",
            "Article text saved for blackassign0068\n",
            "Article text saved for blackassign0069\n",
            "Article text saved for blackassign0070\n",
            "Article text saved for blackassign0071\n",
            "Article text saved for blackassign0072\n",
            "Article text saved for blackassign0073\n",
            "Article text saved for blackassign0074\n",
            "Article text saved for blackassign0075\n",
            "Article text saved for blackassign0076\n",
            "Article text saved for blackassign0077\n",
            "Article text saved for blackassign0078\n",
            "Article text saved for blackassign0079\n",
            "Article text saved for blackassign0080\n",
            "Article text saved for blackassign0081\n",
            "Article text saved for blackassign0082\n",
            "Article text saved for blackassign0083\n",
            "Article text saved for blackassign0084\n",
            "Article text saved for blackassign0085\n",
            "Article text saved for blackassign0086\n",
            "Article text saved for blackassign0087\n",
            "Article text saved for blackassign0088\n",
            "Article text saved for blackassign0089\n",
            "Article text saved for blackassign0090\n",
            "Article text saved for blackassign0091\n",
            "Article text saved for blackassign0092\n",
            "Article text saved for blackassign0093\n",
            "Article text saved for blackassign0094\n",
            "Article text saved for blackassign0095\n",
            "Article text saved for blackassign0096\n",
            "Article text saved for blackassign0097\n",
            "Article text saved for blackassign0098\n",
            "Article text saved for blackassign0099\n",
            "Article text saved for blackassign0100\n",
            "Extraction completed.\n"
          ]
        }
      ],
      "source": [
        "!pip install newspaper3k\n",
        "!pip install textblob\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from newspaper import Article\n",
        "from textblob import TextBlob\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import cmudict\n",
        "\n",
        "nltk.download('cmudict')\n",
        "cmu_dict = cmudict.dict()\n",
        "nltk.download('punkt')\n",
        "\n",
        "#Paths\n",
        "path_input   ='/content/Input.xlsx'\n",
        "path_positive='/content/positive-words.txt'\n",
        "path_negative='/content/negative-words.txt'\n",
        "path_stop = [\n",
        "    '/content/StopWords_Auditor.txt',\n",
        "    '/content/StopWords_Currencies.txt',\n",
        "    '/content/StopWords_DatesandNumbers.txt',\n",
        "    '/content/StopWords_Generic.txt',\n",
        "    '/content/StopWords_Geographic.txt',\n",
        "    '/content/StopWords_Names.txt',\n",
        "    '/content/StopWords_GenericLong.txt'\n",
        "]\n",
        "\n",
        "# Read\n",
        "df = pd.read_excel(path_input)\n",
        "\n",
        "def extract_article_text(url):  # Function to extract article text from URL\n",
        "    try:\n",
        "        article = Article(url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "\n",
        "        title = article.title  # Title\n",
        "        article_text = article.text  # Text\n",
        "        return title, article_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting article from {url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Save text files\n",
        "if not os.path.exists('Article_Text'):\n",
        "    os.makedirs('Article_Text')\n",
        "\n",
        "# Dictionary to store URLs that worked\n",
        "url_dict = {}\n",
        "\n",
        "# Iterate over each row in the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "\n",
        "    title, article_text = extract_article_text(url)  # Extract article text\n",
        "\n",
        "    url_id_clean = re.sub(r'[^\\w]', '_', url_id)  # Replace invalid characters with '_'\n",
        "\n",
        "    file_path = f'Article_Text/{url_id_clean}.txt'\n",
        "    if title and article_text:  # Save\n",
        "        with open(file_path, 'w', encoding='utf-8') as file:\n",
        "            file.write(f\"Title: {title}\\n\\n\")\n",
        "            file.write(article_text)\n",
        "            print(f\"Article text saved for {url_id_clean}\")\n",
        "        url_dict[url_id_clean] = url  # Store the URL\n",
        "    else:\n",
        "        # Create an empty file\n",
        "        with open(file_path, 'w', encoding='utf-8') as file:\n",
        "            print(f\"Unable to extract article text for {url_id_clean}, created empty file.\")\n",
        "\n",
        "print(\"Extraction completed.\")\n",
        "\n",
        "def load_words(file_paths, encoding='ISO-8859-1'):\n",
        "    words = set()\n",
        "    for file_path in file_paths:\n",
        "        with open(file_path, 'r', encoding=encoding) as file:\n",
        "            words.update(file.read().splitlines())\n",
        "    return words\n",
        "\n",
        "# Load positive, negative, and stop words\n",
        "positive_words = load_words([path_positive], encoding='latin-1')\n",
        "negative_words = load_words([path_negative], encoding='latin-1')\n",
        "stop_words = load_words([file_path for file_path in path_stop])\n",
        "\n",
        "# Count syllables in a word\n",
        "def syllable_count(word):\n",
        "    if word.lower() in cmu_dict:\n",
        "        return [len(list(y for y in x if y[-1].isdigit())) for x in cmu_dict[word.lower()]][0]\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Fog index\n",
        "def fog_index(avg_sentence_length, percentage_complex_words):\n",
        "    return 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "# Linguistic features\n",
        "def compute_features(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    num_sentences = len(sentences)\n",
        "    num_words = len(words)\n",
        "    avg_sentence_length = num_words / num_sentences\n",
        "    complex_word_count = sum(1 for word in words if syllable_count(word) > 2)\n",
        "    percentage_complex_words = (complex_word_count / num_words) * 100\n",
        "\n",
        "    fog_index_value = fog_index(avg_sentence_length, percentage_complex_words)\n",
        "\n",
        "    avg_words_per_sentence = num_words / num_sentences\n",
        "    avg_word_length = sum(len(word) for word in words) / num_words\n",
        "\n",
        "    personal_pronouns = sum(1 for word in words if word.lower() in ['i', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves'])\n",
        "    syllables_per_word = sum(syllable_count(word) for word in words) / num_words\n",
        "\n",
        "    # Sentiment analysis using TextBlob\n",
        "    positive_score = sum(1 for word in words if word in positive_words)\n",
        "    negative_score = sum(1 for word in words if word in negative_words)\n",
        "    sentiment = TextBlob(text)\n",
        "    polarity_score = sentiment.sentiment.polarity\n",
        "    subjectivity_score = sentiment.sentiment.subjectivity\n",
        "\n",
        "    return positive_score, negative_score, polarity_score, subjectivity_score, avg_sentence_length, percentage_complex_words, fog_index_value, avg_words_per_sentence, complex_word_count, num_words, syllables_per_word, personal_pronouns, avg_word_length\n",
        "\n",
        "# Read the extracted article texts\n",
        "article_texts = {}\n",
        "for filename in os.listdir('Article_Text'):\n",
        "    with open(os.path.join('Article_Text', filename), 'r', encoding='utf-8') as file:\n",
        "        article_texts[filename.split('.')[0]] = file.read()\n",
        "\n",
        "# Initialize lists to store computed features\n",
        "url_ids = []\n",
        "urls = []\n",
        "positive_scores = []\n",
        "negative_scores = []\n",
        "polarity_scores = []\n",
        "subjectivity_scores = []\n",
        "avg_sentence_lengths = []\n",
        "percentage_complex_words_list = []\n",
        "fog_indexes = []\n",
        "avg_words_per_sentence_list = []\n",
        "complex_word_counts = []\n",
        "word_counts = []\n",
        "syllables_per_word_list = []\n",
        "personal_pronouns_list = []\n",
        "avg_word_lengths = []\n",
        "\n",
        "# Compute features for each article text\n",
        "for url_id, text in article_texts.items():\n",
        "    url_ids.append(url_id)\n",
        "    urls.append(url_dict.get(url_id, \"N/A\"))\n",
        "\n",
        "    if \"Title:\" in text:\n",
        "        positive_score, negative_score, polarity_score, subjectivity_score, avg_sentence_length, percentage_complex_words, fog_index_value, avg_words_per_sentence, complex_word_count, num_words, syllables_per_word, personal_pronouns, avg_word_length = compute_features(text)\n",
        "\n",
        "        positive_scores.append(positive_score)\n",
        "        negative_scores.append(negative_score)\n",
        "        polarity_scores.append(round(polarity_score,3))\n",
        "        subjectivity_scores.append(round(subjectivity_score,3))\n",
        "        avg_sentence_lengths.append(round(avg_sentence_length,3))\n",
        "        percentage_complex_words_list.append(round(percentage_complex_words,3))\n",
        "        fog_indexes.append(round(fog_index_value,3))\n",
        "        avg_words_per_sentence_list.append(round(avg_words_per_sentence,3))\n",
        "        complex_word_counts.append(complex_word_count)\n",
        "        word_counts.append(num_words)\n",
        "        syllables_per_word_list.append(round(syllables_per_word,5))\n",
        "        personal_pronouns_list.append(personal_pronouns)\n",
        "        avg_word_lengths.append(round(avg_word_length,5))\n",
        "    else:\n",
        "        positive_scores.append(\"error loading site\")\n",
        "        negative_scores.append(None)\n",
        "        polarity_scores.append(None)\n",
        "        subjectivity_scores.append(None)\n",
        "        avg_sentence_lengths.append(None)\n",
        "        percentage_complex_words_list.append(None)\n",
        "        fog_indexes.append(None)\n",
        "        avg_words_per_sentence_list.append(None)\n",
        "        complex_word_counts.append(None)\n",
        "        word_counts.append(None)\n",
        "        syllables_per_word_list.append(None)\n",
        "        personal_pronouns_list.append(None)\n",
        "        avg_word_lengths.append(None)\n",
        "\n",
        "# Create a DataFrame to store the computed features\n",
        "output_df = pd.DataFrame({\n",
        "    'URL_ID': url_ids,\n",
        "    'URL': urls,\n",
        "    'POSITIVE SCORE': positive_scores,\n",
        "    'NEGATIVE SCORE': negative_scores,\n",
        "    'POLARITY SCORE': polarity_scores,\n",
        "    'SUBJECTIVITY SCORE': subjectivity_scores,\n",
        "    'AVG SENTENCE LENGTH': avg_sentence_lengths,\n",
        "    'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words_list,\n",
        "    'FOG INDEX': fog_indexes,\n",
        "    'AVG NUMBER OF WORDS PER SENTENCE': avg_words_per_sentence_list,\n",
        "    'COMPLEX WORD COUNT': complex_word_counts,\n",
        "    'WORD COUNT': word_counts,\n",
        "    'SYLLABLE PER WORD': syllables_per_word_list,\n",
        "    'PERSONAL PRONOUNS': personal_pronouns_list,\n",
        "    'AVG WORD LENGTH': avg_word_lengths\n",
        "})\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "output_df.to_excel('Text_Analysis_Output.xlsx', index=False)\n"
      ]
    }
  ]
}